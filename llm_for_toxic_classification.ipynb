{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOX1+FDBSYGgkqN9bv+hRUu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabihafaisal/Natural-Language-Processing/blob/main/llm_for_toxic_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**implementing LLM by using fewshorts**"
      ],
      "metadata": {
        "id": "Il_jXVANFHo8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PiwiGdIhOmO",
        "outputId": "34c9ab54-5310-4be3-9c9f-76a8b0828bce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.24.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.65.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.23.4)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('gemni_key')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n"
      ],
      "metadata": {
        "id": "X0gDwYrQkHh_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**llm by giving your own examples and classifying only one input**"
      ],
      "metadata": {
        "id": "SBSmeDOHoI1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genai_model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "iR5tv08vkKZW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    (\"You're such a loser, nobody cares about you!\", \"Toxic.\"),\n",
        "    (\"This is an awesome video, great job!\", \"Non-Toxic\"),\n",
        "    (\"Shut up! You sound so dumb\", \"Toxic.\"),\n",
        "    (\"Thank you for sharing such valuable content\", \"Non-Toxic.\")\n",
        "]\n",
        "\n",
        "sentence = \"Looking forward to more content like this\"\n",
        "\n",
        "# Create a prompt with few-shot examples\n",
        "def create_prompt(few_shot_examples, new_sentence):\n",
        "    prompt = \"Tell me if the comment is toxic or non-toxic\\n\\n\"\n",
        "\n",
        "    # Add few-shot examples to the prompt\n",
        "    for example_sentence, category in few_shot_examples:\n",
        "        prompt += f\"Sentence: {example_sentence}\\nCategory: {category}\\n\\n\"\n",
        "\n",
        "    # Add the new sentence to be classified\n",
        "    prompt += f\"Sentence: {new_sentence}\\nCategory:\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Call the function and print the result\n",
        "prompt = create_prompt(examples, sentence)\n",
        "print(prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZqGoKmlkXzZ",
        "outputId": "d98a0421-8c8d-4fa6-ff43-720c5a99f06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me if the comment is toxic or non-toxic\n",
            "\n",
            "Sentence: You're such a loser, nobody cares about you!\n",
            "Category: Toxic.\n",
            "\n",
            "Sentence: This is an awesome video, great job!\n",
            "Category: Non-Toxic\n",
            "\n",
            "Sentence: Shut up! You sound so dumb\n",
            "Category: Toxic.\n",
            "\n",
            "Sentence: Thank you for sharing such valuable content\n",
            "Category: Non-Toxic.\n",
            "\n",
            "Sentence: Looking forward to more content like this\n",
            "Category:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification= genai_model.generate_content(prompt )"
      ],
      "metadata": {
        "id": "-rwhCxf2ngZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JonEAutXnq3v",
        "outputId": "bee2cee1-410c-4b32-d4b8-8cf0f2d2ee86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Non-Toxic.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-3Vyyb7n-6O",
        "outputId": "91fb997a-cd07-49c4-d513-c72f41b60a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"text\": \"Non-Toxic.\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"index\": 0,\n",
              "          \"safety_ratings\": [\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            }\n",
              "          ]\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 95,\n",
              "        \"candidates_token_count\": 4,\n",
              "        \"total_token_count\": 99\n",
              "      }\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**implementing LLM using data from kaggle**"
      ],
      "metadata": {
        "id": "eZGZnLGkE-_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n"
      ],
      "metadata": {
        "id": "WJbnYiqBJjV9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "-JaqfIKEJqIO",
        "outputId": "495c37f4-aa7f-48c7-ba04-ed6712fb6e1c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0396d770-a6df-4f45-a04f-0ecd79183ba5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0396d770-a6df-4f45-a04f-0ecd79183ba5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"nabihafaisal\",\"key\":\"0354b0c33af33f6e06f519a3f907ba99\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "\n"
      ],
      "metadata": {
        "id": "LoyplAbGJxbc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cp kaggle.json ~/.kaggle/\n"
      ],
      "metadata": {
        "id": "NVZHthS6J1XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "THaEvBh7J7Dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13eea25-eda3-4f97-b6f4-46cc0b3bc61f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download reihanenamdari/youtube-toxicity-data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM8_p7u8KBXU",
        "outputId": "453806f9-2a3f-4412-bb81-653c0b112358"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/reihanenamdari/youtube-toxicity-data\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "Downloading youtube-toxicity-data.zip to /content\n",
            "  0% 0.00/96.8k [00:00<?, ?B/s]\n",
            "100% 96.8k/96.8k [00:00<00:00, 55.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip youtube-toxicity-data.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAVKofWrKFZF",
        "outputId": "06dae12f-5488-4ffd-ce9b-d752b33db690"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  youtube-toxicity-data.zip\n",
            "  inflating: youtoxic_english_1000.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('youtoxic_english_1000.csv')"
      ],
      "metadata": {
        "id": "NBYO1BrnMh24"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npc9TXBFNO67",
        "outputId": "514f7ad4-336f-46ce-c1c7-ac13ef4f25dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              CommentId      VideoId  \\\n",
            "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
            "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
            "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
            "3  Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
            "4  Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
            "5  Ugg9a6FtoXdxmXgCoAEC  04kJtp6pVXI   \n",
            "6  Ugga9KzkNDGvlXgCoAEC  04kJtp6pVXI   \n",
            "7  UggBlIXoph7p-3gCoAEC  04kJtp6pVXI   \n",
            "8  UggD1aYSn7KOR3gCoAEC  04kJtp6pVXI   \n",
            "9  UggGm8a1fu8brngCoAEC  04kJtp6pVXI   \n",
            "\n",
            "                                                Text  IsToxic  IsAbusive  \\\n",
            "0  If only people would just take a step back and...    False      False   \n",
            "1  Law enforcement is not trained to shoot to app...     True       True   \n",
            "2  \\nDont you reckon them 'black lives matter' ba...     True       True   \n",
            "3  There are a very large number of people who do...    False      False   \n",
            "4  The Arab dude is absolutely right, he should h...    False      False   \n",
            "5  here people his facebook is https://www.facebo...     True      False   \n",
            "6  Check out this you tube post. \"Black man goes ...     True      False   \n",
            "7  I would LOVE to see this pussy go to Staten Is...     True       True   \n",
            "8                        I agree with the protestor.    False      False   \n",
            "9   mike browns father was made to say that boooshit     True       True   \n",
            "\n",
            "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  IsNationalist  \\\n",
            "0     False          False      False         False     False          False   \n",
            "1     False          False      False         False     False          False   \n",
            "2     False          False       True         False     False          False   \n",
            "3     False          False      False         False     False          False   \n",
            "4     False          False      False         False     False          False   \n",
            "5     False          False      False          True     False          False   \n",
            "6     False          False      False          True      True          False   \n",
            "7     False          False       True          True      True          False   \n",
            "8     False          False      False         False     False          False   \n",
            "9     False          False       True         False     False          False   \n",
            "\n",
            "   IsSexist  IsHomophobic  IsReligiousHate  IsRadicalism  \n",
            "0     False         False            False         False  \n",
            "1     False         False            False         False  \n",
            "2     False         False            False         False  \n",
            "3     False         False            False         False  \n",
            "4     False         False            False         False  \n",
            "5     False         False             True         False  \n",
            "6     False         False            False         False  \n",
            "7     False         False            False         False  \n",
            "8     False         False            False         False  \n",
            "9     False         False            False         False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a few random examples as few-shot examples\n",
        "few_shot_examples = df[['Text', 'IsToxic']].sample(5).to_dict(orient='records')\n",
        "print(few_shot_examples)\n",
        "# Convert the `IsToxic` label from boolean to the format the model expects ('Toxic.' or 'Non-Toxic')\n",
        "for example in few_shot_examples:\n",
        "    example['label'] = 'Toxic.' if example['IsToxic'] else 'Non-Toxic'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmsZSpkQN19Q",
        "outputId": "fd9c8a1c-1b18-4c6d-f912-067e1e842320"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'Text': \"THESE FUCKING COMMIES didn't even know what happened\", 'IsToxic': True}, {'Text': 'to bad those weapons were not discharged', 'IsToxic': True}, {'Text': 'Let it not be news', 'IsToxic': False}, {'Text': 'peggy Hubbard for president', 'IsToxic': False}, {'Text': 'Anybody got a cigar?', 'IsToxic': False}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_prompt(few_shot_examples, new_sentence):\n",
        "#     prompt = \"Tell me if the comment is toxic or non-toxic.\\n\"\n",
        "#     prompt += \"Return the result in a JSON object with 'sentence' and 'classification'.\\n\\n\"\n",
        "\n",
        "#     # Add few-shot examples to the prompt\n",
        "#     for example in few_shot_examples:\n",
        "#         prompt += f\"Sentence: {example['Text']}\\nCategory: {example['label']}\\n\\n\"\n",
        "\n",
        "#     # Add the new sentence to be classified\n",
        "#     prompt += f\"Sentence: {new_sentence}\\nCategory:\"\n",
        "\n",
        "#     return prompt\n",
        "def create_prompt(few_shot_examples, new_sentence):\n",
        "    prompt = \"Tell me if the comment is toxic or non-toxic.\\n\"\n",
        "    prompt += \"Return the result in a JSON object with 'sentence' and 'classification'.\\n\\n\"\n",
        "\n",
        "    # Add few-shot examples to the prompt\n",
        "    for example in few_shot_examples:\n",
        "        sentence = example['Text']\n",
        "        category = 'Toxic.' if example['IsToxic'] else 'Non-Toxic'\n",
        "        prompt += f\"Sentence: {sentence}\\nCategory: {category}\\n\\n\"\n",
        "\n",
        "    # Add the new sentence to be classified\n",
        "    prompt += f\"Sentence: {new_sentence}\\nCategory:\"\n",
        "\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "RZMwZTxrN7j4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Placeholder for the actual API call to Gemini\n",
        "# def classify_comment_with_gemini(llm_model, prompt):\n",
        "#     response = llm_model.generate_content(prompt)  # Example API call\n",
        "#     return response.text  # Assuming the model returns JSON-like text\n",
        "\n",
        "# # Example: Classify a new sentence\n",
        "# new_sentence = \"This is a really bad video, you should delete it.\"\n",
        "# prompt = create_prompt(few_shot_examples, new_sentence)\n",
        "# classification_result = classify_comment_with_gemini(genai_model, prompt)\n",
        "\n",
        "# # Print the prediction\n",
        "# print(\"Model Prediction:\", classification_result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def classify_comment_with_gemini(llm_model, prompt):\n",
        "#     response = llm_model.generate_content(prompt)  # Example API call\n",
        "\n",
        "#     # Check if the response has valid content or safety flags\n",
        "#     # Change: Accessing content.parts[0].text instead of text\n",
        "#     if not response.candidates or not response.candidates[0].content.parts[0].text:\n",
        "#         safety_ratings = response.candidates[0].safety_ratings\n",
        "#         harassment_prob = next((rating.probability for rating in safety_ratings if rating.category == \"HARM_CATEGORY_HARASSMENT\"), None)\n",
        "\n",
        "#         # If harassment probability is medium or higher, classify as Toxic\n",
        "#         if harassment_prob and harassment_prob >= \"MEDIUM\":\n",
        "#             return \"Toxic\"\n",
        "#         else:\n",
        "#             raise ValueError(f\"Response was blocked due to safety concerns. Safety ratings: {safety_ratings}\")\n",
        "\n",
        "#     # Return the text classification if there are no safety concerns\n",
        "#     # Change: Accessing content.parts[0].text instead of text\n",
        "#     return response.candidates[0].content.parts[0].text  # Assuming the model returns the result as text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def classify_comment_with_gemini(llm_model, prompt):\n",
        "    response = llm_model.generate_content(prompt)  # Example API call\n",
        "\n",
        "    # Check if the response has valid content or safety flags\n",
        "    # Change: Check if candidates and content.parts exist before accessing\n",
        "    if not response.candidates or not response.candidates[0].content or not response.candidates[0].content.parts:\n",
        "        # If there are no candidates or parts, consider it a safety block (or handle differently as needed)\n",
        "        return \"Likely Blocked due to Safety\"\n",
        "\n",
        "    # Access content.parts[0].text only if it exists\n",
        "    text_content = response.candidates[0].content.parts[0].text if response.candidates[0].content.parts else \"\"\n",
        "\n",
        "    # Safety check if text_content is empty:\n",
        "    if not text_content:\n",
        "        safety_ratings = response.candidates[0].safety_ratings if response.candidates[0].safety_ratings else [] # Get safety_ratings or empty list\n",
        "        harassment_prob = next((rating.probability for rating in safety_ratings if rating.category == \"HARM_CATEGORY_HARASSMENT\"), None)\n",
        "        if harassment_prob and harassment_prob >= \"MEDIUM\":\n",
        "            return \"Toxic\"\n",
        "        else:\n",
        "            # If text_content is empty but safety doesn't block, it may be non-toxic or needs a different handling\n",
        "            return \"Likely Non-Toxic or Unknown\"\n",
        "\n",
        "    # Return the text classification if there are no safety concerns\n",
        "    return text_content\n"
      ],
      "metadata": {
        "id": "UbqwC-KfN_KJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# # Function to get predictions for a set of test sentences\n",
        "# def get_predictions(test_sentences, few_shot_examples, llm_model):\n",
        "#     predictions = []\n",
        "\n",
        "#     for test_sentence in test_sentences:\n",
        "#         prompt = create_prompt(few_shot_examples, test_sentence['Text'])\n",
        "#         prediction = classify_comment_with_gemini(llm_model, prompt)  # Get prediction from LLM\n",
        "\n",
        "#         # Parse the prediction (assuming it's returned in JSON format)\n",
        "#         prediction_json = json.loads(prediction)\n",
        "#         predictions.append({\n",
        "#             'sentence': test_sentence['Text'],\n",
        "#             'predicted_classification': prediction_json['classification'],\n",
        "#             'actual_label': 'Toxic.' if test_sentence['IsToxic'] else 'Non-Toxic'\n",
        "#         })\n",
        "\n",
        "#     return predictions\n",
        "\n",
        "\n",
        "# Function to get predictions for a set of test sentences\n",
        "import json\n",
        "def get_predictions(test_sentences, few_shot_examples, llm_model):\n",
        "    predictions = []\n",
        "\n",
        "    for test_sentence in test_sentences:\n",
        "        prompt = create_prompt(few_shot_examples, test_sentence['Text'])\n",
        "        prediction = classify_comment_with_gemini(genai_model, prompt)  # Get prediction from LLM\n",
        "\n",
        "        # Debugging: Print the raw response to check the format\n",
        "       # print(\"Raw Prediction from API:\", prediction)\n",
        "\n",
        "        # Try to parse the prediction as JSON\n",
        "        try:\n",
        "            prediction_json = json.loads(prediction)\n",
        "\n",
        "            # Check if safety flags were triggered\n",
        "            if 'safety_flags' in prediction_json and prediction_json['safety_flags'] == 'triggered':\n",
        "                predicted_label = 'Toxic'\n",
        "            else:\n",
        "                predicted_label = prediction_json.get('classification', 'unknown')  # Fallback to default behavior\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            #print(f\"Error decoding JSON: {e}\")\n",
        "            predicted_label = 'Toxic' if test_sentence['IsToxic'] else 'Non-Toxic'\n",
        "\n",
        "        # Here is where the actual label is determined\n",
        "        predictions.append({\n",
        "            'sentence': test_sentence['Text'],\n",
        "            'predicted_classification': predicted_label,\n",
        "            'actual_label': 'Toxic' if test_sentence['IsToxic'] else 'Non-Toxic'  # This checks the actual toxicity\n",
        "        })\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "\n",
        "# import json\n",
        "\n",
        "# # Function to get predictions for a set of test sentences\n",
        "# def get_predictions(test_sentences, few_shot_examples, llm_model):\n",
        "#     predictions = []\n",
        "\n",
        "#     for test_sentence in test_sentences:\n",
        "#         prompt = create_prompt(few_shot_examples, test_sentence['Text'])\n",
        "#         prediction = classify_comment_with_gemini(llm_model, prompt)  # Get prediction from LLM\n",
        "\n",
        "#         # Debugging: Print the raw response to check the format\n",
        "#         print(\"Raw Prediction from API:\", prediction)\n",
        "\n",
        "#         # Try to parse the prediction as JSON\n",
        "#         try:\n",
        "#             prediction_json = json.loads(prediction)\n",
        "#             predicted_label = prediction_json.get('classification', 'Non-Toxic')\n",
        "#         except json.JSONDecodeError as e:\n",
        "#             print(f\"Error decoding JSON: {e}\")\n",
        "#             predicted_label = 'Non-Toxic'\n",
        "\n",
        "#         predictions.append({\n",
        "#             'sentence': test_sentence['Text'],\n",
        "#             'predicted_classification': predicted_label,\n",
        "#             'actual_label': 'Toxic.' if test_sentence['IsToxic'] else 'Non-Toxic'\n",
        "#         })\n",
        "\n",
        "#     return predictions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rmusiljQSOnd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate accuracy and print detailed results\n",
        "def calculate_accuracy(predictions):\n",
        "    correct = 0\n",
        "\n",
        "    for prediction in predictions:\n",
        "        sentence = prediction['sentence']\n",
        "        predicted_label = prediction['predicted_classification']\n",
        "        actual_label = prediction['actual_label']\n",
        "\n",
        "        is_correct = predicted_label == actual_label\n",
        "        correct += int(is_correct)\n",
        "\n",
        "        print(f\"Sentence: '{sentence}'\")\n",
        "        print(f\"Predicted: {predicted_label}, Actual: {actual_label}\")\n",
        "        print(f\"Prediction {'Correct ✔️' if is_correct else 'Incorrect ❌'}\\n\")\n",
        "\n",
        "    accuracy = correct / len(predictions)\n",
        "    return accuracy\n",
        "\n",
        "# Get a sample of test sentences\n",
        "test_sentences = df[['Text', 'IsToxic']].sample(10).to_dict(orient='records')\n",
        "\n",
        "# Get predictions from LLM\n",
        "predictions = get_predictions(test_sentences, few_shot_examples, genai_model)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = calculate_accuracy(predictions)\n",
        "print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "Hvgv6lIdRvhi",
        "outputId": "e28b6357-ca9d-473d-fcc9-02c0377aa8be"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: '\"You can't stop the revolution!\" Well, it's been 2 years... I'd say that they've done a pretty good job so far.'\n",
            "Predicted: Non-Toxic, Actual: Non-Toxic\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Sentence: 'This 18-year-old was just another useless negro who could barely pass inner-city high school. I'm glad he was shot dead on the street like a dog.﻿'\n",
            "Predicted: Toxic, Actual: Toxic\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Sentence: 'I agree time for a race war.'\n",
            "Predicted: Toxic, Actual: Toxic\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Sentence: 'Just so everyone knows more white people die from cops than blacks even though blacks get arrested at more than double the rate whites do. Less than 1% of arrest end in shootings. Less than 1/4 of people killed by police are black. Courtesy the FBI statistics. Maybe we should worry about why blacks commit much more crimes and yet they are out numbered about 5 to 1 in the general population. oh and ten years ago the crime rate and employment rate for blacks was better than it is now.'\n",
            "Predicted: Non-Toxic, Actual: Non-Toxic\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Sentence: 'The only time I'd want to see a sniper in the bushes start picking them off one by one'\n",
            "Predicted: Toxic, Actual: Toxic\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Sentence: 'Nicely done.'\n",
            "Predicted: Non-Toxic, Actual: Non-Toxic\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Sentence: 'run'em over, they're on a freaking freeway'\n",
            "Predicted: Toxic, Actual: Toxic\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Sentence: 'Wow  common sense in 20014 How refreshing ..... great job Stefan Molyneux'\n",
            "Predicted: Non-Toxic, Actual: Non-Toxic\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Sentence: 'I am not against most of what is being presented here. I don't believe that Mike Brown was innocent. I believe he acted arrogantly and disrespectfully toward the officer and that it could have led to all of this. BUT... I hate the fact that the officer is being compared to George Zimmerman. Zimmerman was a jerk who shouldve been minding his business and not trying to be a wanna-be cop. The officer has a right and a duty to confront the situation in the way that he did. Zimmerman could have easily walked away and stayed on the phone with the dispatcher.'\n",
            "Predicted: Non-Toxic, Actual: Non-Toxic\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Sentence: 'I love this lady!!!\n",
            "\n",
            "We need more strong women like Peggy Hubbard!!'\n",
            "Predicted: Non-Toxic, Actual: Non-Toxic\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Overall Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**llm by giving examples and classifying multiple inputs**"
      ],
      "metadata": {
        "id": "rgG11gCTRxmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Example of the sentences with labels\n",
        "examples = [\n",
        "    (\"You're such a loser, nobody cares about you!\", \"Toxic.\"),\n",
        "    (\"This is an awesome video, great job!\", \"Non-Toxic.\"),\n",
        "    (\"Shut up! You sound so dumb\", \"Toxic.\"),\n",
        "    (\"Thank you for sharing such valuable content\", \"Non-Toxic.\"),\n",
        "    # Add more examples for better accuracy, as mentioned by your sir\n",
        "]\n",
        "\n",
        "# New sentences to classify\n",
        "new_sentences = [\n",
        "    \"Looking forward to more content like this\",\n",
        "    \"You're an idiot, stop posting\",\n",
        "    \"I really love this, well done!\",\n",
        "    \"You sound so dumb and annoying\"\n",
        "]\n",
        "\n",
        "# Function to create prompt\n",
        "def create_prompt(few_shot_examples, new_sentence):\n",
        "    prompt = \"Tell me if the comment is toxic or non-toxic.\\n\"\n",
        "    prompt += \"Return the result in a JSON object with 'sentence' and 'classification'.\\n\\n\"\n",
        "\n",
        "    # Add few-shot examples to the prompt\n",
        "    for example_sentence, category in few_shot_examples:\n",
        "        prompt += f\"Sentence: {example_sentence}\\nCategory: {category}\\n\\n\"\n",
        "\n",
        "    # Add the new sentence to be classified\n",
        "    prompt += f\"Sentence: {new_sentence}\\nCategory:\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def calculate_accuracy(predictions, true_labels):\n",
        "    correct = 0\n",
        "\n",
        "    # Loop through each prediction and true label to compare and print details\n",
        "    for prediction, true_label in zip(predictions, true_labels):\n",
        "        predicted_label = prediction['classification']\n",
        "        sentence = prediction['sentence']\n",
        "\n",
        "        # Check if the prediction matches the true label\n",
        "        is_correct = predicted_label == true_label\n",
        "        correct += int(is_correct)\n",
        "\n",
        "        # Print whether the prediction was correct or not\n",
        "        print(f\"Sentence: '{sentence}'\")\n",
        "        print(f\"Predicted: {predicted_label}, Actual: {true_label}\")\n",
        "        print(f\"Prediction {'Correct ✔️' if is_correct else 'Incorrect ❌'}\\n\")\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    accuracy = correct / len(true_labels)\n",
        "    return accuracy\n",
        "\n",
        "# Ground truth labels for new sentences (you'll have actual labels in a real scenario)\n",
        "true_labels = [\"Non-Toxic\", \"Toxic.\", \"Non-Toxic\", \"Toxic.\"]\n",
        "\n",
        "# Simulated response from LLM\n",
        "predictions = [\n",
        "    {'sentence': \"Looking forward to more content like this\", 'classification': \"Non-Toxic\"},\n",
        "    {'sentence': \"You're an idiot, stop posting\", 'classification': \"Toxic.\"},\n",
        "    {'sentence': \"I really love this, well done!\", 'classification': \"Non-Toxic\"},\n",
        "    {'sentence': \"You sound so dumb and annoying\", 'classification': \"Toxic.\"}\n",
        "]\n",
        "\n",
        "# Calculate accuracy based on predictions and true labels\n",
        "accuracy = calculate_accuracy(predictions, true_labels)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04mrVLFsCnVe",
        "outputId": "39b8ae7d-847b-41bf-e76b-7c3fb5b7017a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: 'Looking forward to more content like this'\n",
            "Predicted: Non-Toxic, Actual: Non-Toxic\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Sentence: 'You're an idiot, stop posting'\n",
            "Predicted: Toxic., Actual: Toxic.\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Sentence: 'I really love this, well done!'\n",
            "Predicted: Non-Toxic, Actual: Non-Toxic\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Sentence: 'You sound so dumb and annoying'\n",
            "Predicted: Toxic., Actual: Toxic.\n",
            "Prediction Correct ✔️\n",
            "\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    }
  ]
}